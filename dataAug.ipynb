{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14f8affc-84cf-472e-85a8-c35eaef86787",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil \n",
    "import cv2\n",
    "import json \n",
    "import glob\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43f433b4-7ad1-4227-a012-c6f65a98219b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_person_scale(bounding_box, reference_height=200):\n",
    "    \"\"\"\n",
    "    Calculate the person scale relative to a reference height.\n",
    "\n",
    "    Parameters:\n",
    "    - bounding_box: Tuple or list representing the bounding box coordinates (x, y, width, height).\n",
    "    - reference_height: Reference height for scaling (default is 200 pixels).\n",
    "\n",
    "    Returns:\n",
    "    - Person scale.\n",
    "    \"\"\"\n",
    "    _, _, _, height = bounding_box\n",
    "    person_scale = height / reference_height\n",
    "    return person_scale\n",
    "\n",
    "\n",
    "def augment_image(image, hue_shift, saturation_scale, brightness_scale):\n",
    "    # Convert the image from BGR to HSV\n",
    "    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Apply augmentation\n",
    "    hsv_image[:, :, 0] = (hsv_image[:, :, 0] + hue_shift) % 180\n",
    "    hsv_image[:, :, 1] = np.clip(saturation_scale * hsv_image[:, :, 1], 0, 255)\n",
    "    hsv_image[:, :, 2] = np.clip(brightness_scale * hsv_image[:, :, 2], 0, 255)\n",
    "\n",
    "    # Convert the augmented image back to BGR\n",
    "    augmented_image = cv2.cvtColor(hsv_image, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "    return augmented_image\n",
    "\n",
    "# Set augmentation parameters\n",
    "hue_shifts = [20, -20, 0, 10, 15]  # Degrees to shift the hue\n",
    "saturation_scales = [1.5, 0.5, 1.0, 1.2, 1.4]  # Scaling factor for saturation\n",
    "brightness_scales = [1.2, 1.0, 0.8, 1.0, 1.2]  # Scaling factor for brightness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c0e3b2f-76bb-4ef7-a34f-422e5adcd65e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # File paths\n",
    "# annotations_file_path = './finalProjectCode/tf-keras-simple-baselines-keypoint-detection/data/mpii/annotations.json'\n",
    "\n",
    "# # # Read existing annotations from the file\n",
    "# with open(annotations_file_path, 'r') as file:\n",
    "#     annotations = json.load(file)\n",
    "# print( annotations[0])\n",
    "\n",
    "annotations = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ecafd20-ca48-465f-96f5-712dc45f31bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:11,  4.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:15,  3.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "47it [00:15,  3.07it/s]\n"
     ]
    }
   ],
   "source": [
    "classes = ['lateralRaises', 'pushups', 'squats']\n",
    "\n",
    "fol_dir = './customDataset_org/'\n",
    "output_dir = './customDataset/augImgDataset/'\n",
    "output_dirCustom = './customDataset/totalCustomDataset/'\n",
    "validation_split = 0.3\n",
    "count = 0 \n",
    "for each_cls in classes:\n",
    "    list_imgs = glob.glob( fol_dir + each_cls + '/*/*.json')\n",
    "    print( len(list_imgs))    \n",
    "    for idx, each in tqdm( enumerate(list_imgs)):        \n",
    "        # filename         \n",
    "        with open(each, 'r') as file:\n",
    "            each_ann = json.load(file)\n",
    "        image_path = each_ann['imagePath']\n",
    "        scale = 0 \n",
    "        # # # get the keypoints \n",
    "        cust_ann = {}\n",
    "        cust_ann['joint_self'] = []\n",
    "        cust_ann['dataset'] = 'MPI'\n",
    "        cust_ann['scale_provided'] = scale        \n",
    "        cust_keypoints = []\n",
    "        cust_ann['isValidation'] = 0 if random.random() > validation_split else 1\n",
    "        for each_kp in each_ann['shapes']:            \n",
    "            if each_kp['shape_type'] == \"rectangle\":\n",
    "                [x1, y1, x2, y2] = each_kp['points'][0][0], each_kp['points'][0][1], each_kp['points'][1][0], each_kp['points'][1][1]\n",
    "                bbox = [x1, y1, y2 - y1, x2 - x1]\n",
    "                scale = calculate_person_scale( bbox) \n",
    "                cust_ann['objpos'] = [(x1 + x2//2), (y1 +  y2)//2]\n",
    "                continue\n",
    "            cust_ann['joint_self'].append( [each_kp['points'][0][0], each_kp['points'][0][1], 1])                    \n",
    "        cust_ann['scale_provided'] = scale\n",
    "        #print( len( cust_ann['joint_self']), image_path, each_cls)\n",
    "        diff = 16 - len( cust_ann['joint_self'])\n",
    "        if diff > 0:\n",
    "            while len( cust_ann['joint_self']) != 16:\n",
    "                cust_ann['joint_self'].append( [0, 0, 0])\n",
    "        if diff < 0:\n",
    "            while len( cust_ann['joint_self']) != 16:\n",
    "                cust_ann['joint_self'].pop()        \n",
    "        assert len( cust_ann['joint_self']) == 16            \n",
    "        #original_image = cv2.imread(fol_dir + '/' + each_cls + '/' + image_path)\n",
    "        original_image = cv2.imread(each[:-4] + 'jpg')        \n",
    "        for i, (hue_shift, saturation_scale, brightness_scale) in enumerate(zip(hue_shifts, saturation_scales, brightness_scales)):            \n",
    "            augmented_image = augment_image(original_image, hue_shift, saturation_scale, brightness_scale)\n",
    "            #cv2.imwrite( output_dir + each_cls + '/' + f'augmented_image_{i+1}_{each_cls}_{image_path}.jpg', augmented_image)\n",
    "            cv2.imwrite( output_dirCustom + f'{count}.jpg', augmented_image)\n",
    "            each_file_path =  f'augmented_image_{i+1}_{each_cls}_{image_path}'            \n",
    "            cust_ann['img_paths'] = f'{count}.jpg'\n",
    "            annotations.append(cust_ann)\n",
    "            #print( cust_ann['img_paths'], annotations[count], count)                    \n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8581a795-8a01-450d-be1c-5c0fa83bd111",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary appended and file updated.\n"
     ]
    }
   ],
   "source": [
    "# Write the updated annotations back to the file\n",
    "output_ann_file_path = output_dir + '/annotations.json'\n",
    "with open(output_ann_file_path, 'w') as file:\n",
    "    json.dump(annotations, file, indent=2)\n",
    "print(\"Dictionary appended and file updated.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
